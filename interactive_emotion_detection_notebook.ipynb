{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "generative_ai_disabled": true,
      "authorship_tag": "ABX9TyO1z4BCgEAGe4ddwwoWY4Er",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fazna-kozhipparambil/Interactive-Emotion-Detection/blob/main/interactive_emotion_detection_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "tr3vHOZBO29b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch torchaudio librosa soundfile pydub ipywidgets\n",
        "\n"
      ],
      "metadata": {
        "id": "WvBd4oLfO4BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Import Libraries & Load Models"
      ],
      "metadata": {
        "id": "So8FRhnSO82K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import librosa\n",
        "from IPython.display import display, Javascript\n",
        "import ipywidgets as widgets\n",
        "from google.colab import output\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "import soundfile as sf\n",
        "\n",
        "# Text Emotion Detection\n",
        "\n",
        "text_emotion_detector = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    top_k=None\n",
        ")\n",
        "\n",
        "\n",
        "# Audio Emotion Detection\n",
        "\n",
        "audio_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "audio_labels = [\"neutral\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgusted\",\"surprised\"]\n"
      ],
      "metadata": {
        "id": "mD_s5dMOTzPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Text Emotion Widget"
      ],
      "metadata": {
        "id": "lzEuToykPDmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_text_emotion(text):\n",
        "    results = text_emotion_detector(text)\n",
        "    print(\"Text Emotion Detection Results:\")\n",
        "    for r in results[0]:\n",
        "        print(f\"{r['label']}: {r['score']:.2f}\")\n",
        "\n",
        "text_input = widgets.Text(\n",
        "    value='I am feeling excited today!',\n",
        "    description='Your Text:',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "widgets.interact_manual(detect_text_emotion, text=text_input)\n"
      ],
      "metadata": {
        "id": "IXClDhnjUCYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Audio File Upload Widget"
      ],
      "metadata": {
        "id": "IXLGrCyKPJtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import librosa\n",
        "from google.colab import files\n",
        "\n",
        "# Load feature extractor and model\n",
        "audio_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "audio_labels = [\"neutral\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgusted\",\"surprised\"]\n",
        "\n",
        "# Upload and predict\n",
        "uploaded = files.upload()\n",
        "audio_file = list(uploaded.keys())[0]\n",
        "\n",
        "speech, sr = librosa.load(audio_file, sr=16000)\n",
        "\n",
        "# Prepare inputs\n",
        "inputs = audio_feature_extractor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = audio_model(**inputs).logits\n",
        "\n",
        "predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "print(\"Audio Emotion Detected:\", audio_labels[predicted_id])\n"
      ],
      "metadata": {
        "id": "mPaujy2lfH3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Live Microphone Widget"
      ],
      "metadata": {
        "id": "6D7aJNkLPTuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to convert JS audio to numpy array\n",
        "def js_to_audio(js_audio):\n",
        "    import soundfile as sf\n",
        "    import io\n",
        "    import numpy as np\n",
        "    audio_bytes = base64.b64decode(js_audio.split(',')[1])\n",
        "    audio_data, samplerate = sf.read(io.BytesIO(audio_bytes))\n",
        "    if audio_data.ndim > 1:\n",
        "        audio_data = audio_data[:,0]  # use first channel if stereo\n",
        "    return audio_data, samplerate\n",
        "\n",
        "# Callback to process recorded audio\n",
        "def process_audio(js_audio):\n",
        "    audio_data, sr = js_to_audio(js_audio)\n",
        "    # Resample to 16kHz\n",
        "    import librosa\n",
        "    if sr != 16000:\n",
        "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=16000)\n",
        "        sr = 16000\n",
        "    inputs = audio_feature_extractor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = audio_model(**inputs).logits\n",
        "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "    print(\"Live Microphone Emotion Detected:\", audio_labels[predicted_id])\n",
        "\n",
        "# Register callback\n",
        "output.register_callback('notebook.process_audio', process_audio)\n",
        "\n",
        "# Function to trigger JS recording with start/stop\n",
        "def record_and_detect_audio(_):\n",
        "    display(Javascript(\"\"\"\n",
        "    let stream;\n",
        "    let mediaRecorder;\n",
        "    let chunks = [];\n",
        "\n",
        "    async function startRecording() {\n",
        "        stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
        "        mediaRecorder = new MediaRecorder(stream);\n",
        "        chunks = [];\n",
        "        mediaRecorder.ondataavailable = e => chunks.push(e.data);\n",
        "        mediaRecorder.start();\n",
        "        alert(\"Recording started! Press OK when done speaking.\");\n",
        "    }\n",
        "\n",
        "    async function stopRecording() {\n",
        "        mediaRecorder.stop();\n",
        "        mediaRecorder.onstop = async () => {\n",
        "            const blob = new Blob(chunks, {type:'audio/wav'});\n",
        "            const reader = new FileReader();\n",
        "            reader.onloadend = () => {\n",
        "                google.colab.kernel.invokeFunction('notebook.process_audio', [reader.result], {});\n",
        "            };\n",
        "            reader.readAsDataURL(blob);\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "        };\n",
        "    }\n",
        "\n",
        "    startRecording().then(() => stopRecording());\n",
        "    \"\"\"))\n",
        "\n",
        "# Button to start live microphone recording\n",
        "mic_button = widgets.Button(description=\"Record Live Audio (Start/Stop)\")\n",
        "mic_button.on_click(record_and_detect_audio)\n",
        "display(mic_button)\n",
        "\n"
      ],
      "metadata": {
        "id": "0aYqtbxrX-Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install gradio transformers torch librosa soundfile --quiet\n",
        "\n",
        "# ---- Imports ----\n",
        "import torch\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "import librosa\n",
        "from transformers import HubertForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "\n",
        "# ---- Load model & feature extractor ----\n",
        "MODEL_NAME = \"superb/hubert-base-superb-er\"\n",
        "\n",
        "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
        "model = HubertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Emotion labels\n",
        "audio_labels = list(model.config.id2label.values())\n",
        "\n",
        "# ---- Define prediction function ----\n",
        "def detect_emotion_from_audio(audio):\n",
        "    \"\"\"\n",
        "    Input: audio = tuple (sampling_rate, numpy array from Gradio)\n",
        "    Output: Predicted emotion as string\n",
        "    \"\"\"\n",
        "    sr, audio_data = audio\n",
        "\n",
        "    # Convert stereo to mono if needed\n",
        "    if audio_data.ndim > 1:\n",
        "        audio_data = np.mean(audio_data, axis=1)\n",
        "\n",
        "    # Resample to 16kHz\n",
        "    if sr != 16000:\n",
        "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=16000)\n",
        "        sr = 16000\n",
        "\n",
        "    # Extract features\n",
        "    inputs = feature_extractor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        logits = model(**inputs).logits\n",
        "        predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "        emotion = audio_labels[predicted_id]\n",
        "\n",
        "    return f\"üé≠ Detected Emotion: **{emotion}**\"\n",
        "\n",
        "# ---- Build Gradio App ----\n",
        "demo = gr.Interface(\n",
        "    fn=detect_emotion_from_audio,\n",
        "    inputs=gr.Audio(type=\"numpy\", label=\"üéôÔ∏è Record or upload audio\"),\n",
        "    outputs=gr.Markdown(label=\"Prediction\"),\n",
        "    title=\"üéß Speech Emotion Detection\",\n",
        "    description=\"Record or upload an audio clip ‚Äî the model predicts your emotion (happy, sad, angry, neutral, etc.) using HuBERT.\"\n",
        ")\n",
        "\n",
        "# ---- Launch App ----\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "3hOr9eLxktb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}