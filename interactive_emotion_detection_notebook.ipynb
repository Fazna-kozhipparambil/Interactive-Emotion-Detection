{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "generative_ai_disabled": true,
      "authorship_tag": "ABX9TyNptouxsk9wnFeTg06htAAz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fazna-kozhipparambil/Interactive-Emotion-Detection/blob/main/interactive_emotion_detection_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Install Dependencies"
      ],
      "metadata": {
        "id": "tr3vHOZBO29b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch torchaudio librosa soundfile pydub ipywidgets\n",
        "\n"
      ],
      "metadata": {
        "id": "WvBd4oLfO4BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2: Import Libraries & Load Models"
      ],
      "metadata": {
        "id": "So8FRhnSO82K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
        "import torch\n",
        "import librosa\n",
        "from IPython.display import display, Javascript\n",
        "import ipywidgets as widgets\n",
        "from google.colab import output\n",
        "import numpy as np\n",
        "import base64\n",
        "import io\n",
        "import soundfile as sf\n",
        "\n",
        "# Text Emotion Detection\n",
        "\n",
        "text_emotion_detector = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
        "    top_k=None\n",
        ")\n",
        "\n",
        "\n",
        "# Audio Emotion Detection\n",
        "\n",
        "audio_feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "audio_model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\")\n",
        "audio_labels = [\"neutral\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgusted\",\"surprised\"]\n"
      ],
      "metadata": {
        "id": "mD_s5dMOTzPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext cudf.pandas\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Randomly generated dataset of parking violations-\n",
        "# Define the number of rows\n",
        "num_rows = 1000000\n",
        "\n",
        "states = [\"NY\", \"NJ\", \"CA\", \"TX\"]\n",
        "violations = [\"Double Parking\", \"Expired Meter\", \"No Parking\",\n",
        "              \"Fire Hydrant\", \"Bus Stop\"]\n",
        "vehicle_types = [\"SUBN\", \"SDN\"]\n",
        "\n",
        "# Create a date range\n",
        "start_date = \"2022-01-01\"\n",
        "end_date = \"2022-12-31\"\n",
        "dates = pd.date_range(start=start_date, end=end_date, freq='D')\n",
        "\n",
        "# Generate random data\n",
        "data = {\n",
        "    \"Registration State\": np.random.choice(states, size=num_rows),\n",
        "    \"Violation Description\": np.random.choice(violations, size=num_rows),\n",
        "    \"Vehicle Body Type\": np.random.choice(vehicle_types, size=num_rows),\n",
        "    \"Issue Date\": np.random.choice(dates, size=num_rows),\n",
        "    \"Ticket Number\": np.random.randint(1000000000, 9999999999, size=num_rows)\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Which parking violation is most commonly committed by vehicles from various U.S states?\n",
        "\n",
        "(df[[\"Registration State\", \"Violation Description\"]]  # get only these two columns\n",
        " .value_counts()  # get the count of offences per state and per type of offence\n",
        " .groupby(\"Registration State\")  # group by state\n",
        " .head(1)  # get the first row in each group (the type of offence with the largest count)\n",
        " .sort_index()  # sort by state name\n",
        " .reset_index()\n",
        ")"
      ],
      "metadata": {
        "id": "wH8VSTwqc-uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Text Emotion Widget"
      ],
      "metadata": {
        "id": "lzEuToykPDmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_text_emotion(text):\n",
        "    results = text_emotion_detector(text)\n",
        "    print(\"Text Emotion Detection Results:\")\n",
        "    for r in results[0]:\n",
        "        print(f\"{r['label']}: {r['score']:.2f}\")\n",
        "\n",
        "text_input = widgets.Text(\n",
        "    value='I am feeling excited today!',\n",
        "    description='Your Text:',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "widgets.interact_manual(detect_text_emotion, text=text_input)\n"
      ],
      "metadata": {
        "id": "IXClDhnjUCYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Audio File Upload Widget"
      ],
      "metadata": {
        "id": "IXLGrCyKPJtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_audio_file(_):\n",
        "    uploaded = files.upload()\n",
        "    audio_file = list(uploaded.keys())[0]\n",
        "\n",
        "    speech, sr = librosa.load(audio_file, sr=16000)\n",
        "    inputs = audio_feature_extractor(speech, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = audio_model(**inputs).logits\n",
        "\n",
        "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "    print(\"Uploaded Audio Emotion Detected:\", audio_labels[predicted_id])\n",
        "\n",
        "audio_button = widgets.Button(description=\"Upload Audio (.wav) for Emotion Detection\")\n",
        "audio_button.on_click(detect_audio_file)\n",
        "display(audio_button)\n"
      ],
      "metadata": {
        "id": "9mZmvibnUKVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Live Microphone Widget"
      ],
      "metadata": {
        "id": "6D7aJNkLPTuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to convert JS audio to numpy array\n",
        "def js_to_audio(js_audio):\n",
        "    import soundfile as sf\n",
        "    import io\n",
        "    import numpy as np\n",
        "    audio_bytes = base64.b64decode(js_audio.split(',')[1])\n",
        "    audio_data, samplerate = sf.read(io.BytesIO(audio_bytes))\n",
        "    if audio_data.ndim > 1:\n",
        "        audio_data = audio_data[:,0]  # use first channel if stereo\n",
        "    return audio_data, samplerate\n",
        "\n",
        "# Callback to process recorded audio\n",
        "def process_audio(js_audio):\n",
        "    audio_data, sr = js_to_audio(js_audio)\n",
        "    # Resample to 16kHz\n",
        "    import librosa\n",
        "    if sr != 16000:\n",
        "        audio_data = librosa.resample(audio_data, orig_sr=sr, target_sr=16000)\n",
        "        sr = 16000\n",
        "    inputs = audio_feature_extractor(audio_data, sampling_rate=sr, return_tensors=\"pt\", padding=True)\n",
        "    with torch.no_grad():\n",
        "        logits = audio_model(**inputs).logits\n",
        "    predicted_id = torch.argmax(logits, dim=-1).item()\n",
        "    print(\"Live Microphone Emotion Detected:\", audio_labels[predicted_id])\n",
        "\n",
        "# Register callback\n",
        "output.register_callback('notebook.process_audio', process_audio)\n",
        "\n",
        "# Function to trigger JS recording with start/stop\n",
        "def record_and_detect_audio(_):\n",
        "    display(Javascript(\"\"\"\n",
        "    let stream;\n",
        "    let mediaRecorder;\n",
        "    let chunks = [];\n",
        "\n",
        "    async function startRecording() {\n",
        "        stream = await navigator.mediaDevices.getUserMedia({audio:true});\n",
        "        mediaRecorder = new MediaRecorder(stream);\n",
        "        chunks = [];\n",
        "        mediaRecorder.ondataavailable = e => chunks.push(e.data);\n",
        "        mediaRecorder.start();\n",
        "        alert(\"Recording started! Press OK when done speaking.\");\n",
        "    }\n",
        "\n",
        "    async function stopRecording() {\n",
        "        mediaRecorder.stop();\n",
        "        mediaRecorder.onstop = async () => {\n",
        "            const blob = new Blob(chunks, {type:'audio/wav'});\n",
        "            const reader = new FileReader();\n",
        "            reader.onloadend = () => {\n",
        "                google.colab.kernel.invokeFunction('notebook.process_audio', [reader.result], {});\n",
        "            };\n",
        "            reader.readAsDataURL(blob);\n",
        "            stream.getTracks().forEach(track => track.stop());\n",
        "        };\n",
        "    }\n",
        "\n",
        "    startRecording().then(() => stopRecording());\n",
        "    \"\"\"))\n",
        "\n",
        "# Button to start live microphone recording\n",
        "mic_button = widgets.Button(description=\"Record Live Audio (Start/Stop)\")\n",
        "mic_button.on_click(record_and_detect_audio)\n",
        "display(mic_button)\n",
        "\n"
      ],
      "metadata": {
        "id": "0aYqtbxrX-Ai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}